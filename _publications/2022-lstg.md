---
title: "Long Short-Term Relation Transformer with Global Gating for Video Captioning"
collection: publications
permalink: /publication/2022-lstg
excerpt: 'Liang Li, Xingyu Gao, Jincan Deng, **Yunbin Tu**, Zheng-Jun Zha, Qingming Huang.


[[PDF](https://ieeexplore.ieee.org/document/9741388)]


<h2>2021</h2>
---'

date: 2022-03-24
venue: 'IEEE Transactions on Image Processing (TIP, Impact Factor: 10.6), 31: 2726-2738, March'
---

Video captioning aims to generate a natural language sentence to describe the main content of a video. Since there are multiple objects in videos, taking full exploration of the spatial and temporal relationships among them is  crucial for this task. The previous methods wrap the detected objects as input sequences, and leverage vanilla self-attention or graph neural network to reason about visual relations. This cannot make full use of the spatial and temporal nature of a video,  and suffers from the problems of redundant connections, over-smoothing, and relation ambiguity.  In order to address the above problems, in this paper we construct a long short-term graph (LSTG) that simultaneously captures short-term spatial semantic relations and long-term transformation dependencies. Further,  to perform relational reasoning over the LSTG, we design a global gated graph reasoning module (G3RM), which introduces a global gating based on global context to control information propagation between objects and alleviate relation ambiguity. Finally, by introducing G3RM into Transformer instead of self-attention, we propose the long short-term relation transformer (LSRT) to fully mine objects' relations for caption generation. Experiments on MSVD and MSR-VTT datasets show that the LSRT achieves superior performance compared with state-of-the-art methods. The visualization results indicate that our method alleviates problem of over-smoothing and strengthens the ability of relational reasoning.

[Download paper here](https://ieeexplore.ieee.org/document/9741388)
