---
title: "Enhancing the alignment between target words and corresponding frames for video captioning"
collection: publications
permalink: /publication/2021-tta
excerpt: '**Yunbin Tu**, Chang Zhou, Junjun Guo, Shengxiang Gao, Zhengtao Yu.


[[PDF](https://www.sciencedirect.com/science/article/abs/pii/S0031320320305057) [Code](https://github.com/tuyunbin/Enhancing-the-Alignment-between-Target-Words-and-Corresponding-Frames-for-Video-Captioning)]


<h2>2019</h2>
---'


date: 2020-11-01
venue: 'Pattern Recognition (CCF-B, Regular paper, IF=8.0)'
---

Video captioning aims at translating from a sequence of video frames into a sequence of words with the encoder-decoder framework. Hence, it is critical to align these two different sequences. Most existing methods exploit soft-attention (temporal attention) mechanism to align target words with corresponding frames, where the relevance of them merely depends on the previously generated words (i.e., language context). As we know, however, there is an inherent gap between vision and language, and most of the words in a caption belong to non-visual words (e.g. “a”, “is”, and “in”). Hence, merely with the guidance of the language context, existing temporal attention-based methods cannot exactly align target words with corresponding frames. In order to address this problem, we first introduce pre-detected visual tags from the video to bridge the gap between vision and language. The reason is that visual tags not only belong to textual modality, but also can convey visual information. Then, we present a Textual-Temporal Attention Model (TTA) to exactly align the target words with corresponding frames. The experimental results show that our proposed method outperforms the state-of-the-art methods on two well known datasets, i.e., MSVD and MSR-VTT.


[Download paper here](https://www.sciencedirect.com/science/article/abs/pii/S0031320320305057)
